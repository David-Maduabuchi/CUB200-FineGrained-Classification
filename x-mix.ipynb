{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5140550,"sourceType":"datasetVersion","datasetId":2534241},{"sourceId":13474668,"sourceType":"datasetVersion","datasetId":8137285}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PROTOTYPE","metadata":{}},{"cell_type":"markdown","source":"## ü§ç Cell 1: Environment Variables","metadata":{}},{"cell_type":"code","source":"# Minimal installs: only what we need\n!pip install -q timm==0.9.12 sentence-transformers==2.7.0 wandb==0.17.0 torchmetrics==1.4.0.post0\n\n# Import essentials\nimport os\nimport random\nimport numpy as np\nimport torch\nimport logging\nfrom torch import nn\nimport wandb\n# Environment tweaks\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Seeds for reproducibility (enough for CUB)\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.benchmark = False  # Deterministic\ntorch.backends.cudnn.deterministic = True\n\n# Logging: only to file + console, no spam\nlogging.basicConfig(\n    level=logging.INFO,\n    force=True,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(), logging.FileHandler(\"/kaggle/working/train.log\", mode=\"w\")]\n)\n\n# Device check\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogging.info(f\"PyTorch {torch.__version__} | Device: {device}\")\nif device.type == \"cuda\":\n    logging.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    logging.info(f\"Initial VRAM: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\nelse:\n    logging.warning(\"No GPU detected. Training will be slow.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T13:33:20.823183Z","iopub.execute_input":"2026-02-15T13:33:20.823381Z","iopub.status.idle":"2026-02-15T13:35:26.491716Z","shell.execute_reply.started":"2026-02-15T13:33:20.823360Z","shell.execute_reply":"2026-02-15T13:35:26.491135Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m792.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2026-02-15 13:35:26,449 - INFO - PyTorch 2.6.0+cu124 | Device: cuda\n2026-02-15 13:35:26,487 - INFO - GPU: Tesla T4\n2026-02-15 13:35:26,488 - INFO - Initial VRAM: 0.00 MB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## ü§ç Cell 2: Dataset Definition (Feature Engineering)","metadata":{}},{"cell_type":"code","source":"# ====================== FINAL DATASET & SPLIT CELL (VISION-ONLY) ======================\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as T\n\n# ---------------- CONFIG ----------------\nSEED = 42\nNUM_CLASSES = 200\nIMG_SIZE = 224\nBATCH_SIZE = 48\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# ---------------- PATHS ----------------\nDATA_ROOT = \"/kaggle/input/cub2002011/CUB_200_2011\"\nIMAGES_DIR = os.path.join(DATA_ROOT, \"images\")\nIMAGES_FILE = os.path.join(DATA_ROOT, \"images.txt\")\nLABELS_FILE = os.path.join(DATA_ROOT, \"image_class_labels.txt\")\nBB_FILE = os.path.join(DATA_ROOT, \"bounding_boxes.txt\")\nSPLIT_FILE = os.path.join(DATA_ROOT, \"train_test_split.txt\")\n\n# ---------------- LOAD METADATA ----------------\nimages_df = pd.read_csv(IMAGES_FILE, sep=\" \", header=None, names=[\"img_id\", \"path\"])\nlabels_df = pd.read_csv(LABELS_FILE, sep=\" \", header=None, names=[\"img_id\", \"class_id\"])\nbb_df = pd.read_csv(BB_FILE, sep=\" \", header=None, names=[\"img_id\", \"x\", \"y\", \"width\", \"height\"])\nsplit_df = pd.read_csv(SPLIT_FILE, sep=\" \", header=None, names=[\"img_id\", \"is_train\"])\n\nimg_paths = dict(zip(images_df.img_id, images_df.path))\nlabels = {row.img_id: row.class_id - 1 for row in labels_df.itertuples()}  # 0‚Äì199\nbboxes = {row.img_id: (row.x, row.y, row.width, row.height) for row in bb_df.itertuples()}\n\ntrain_ids = split_df[split_df[\"is_train\"] == 1][\"img_id\"].tolist()\ntest_ids  = split_df[split_df[\"is_train\"] == 0][\"img_id\"].tolist()\n\nprint(f\"Official train samples: {len(train_ids)}\")\nprint(f\"Official test samples:  {len(test_ids)}\")\n\n# ---------------- DATASET ----------------\nclass CUBVisionDataset(Dataset):\n    def __init__(self, image_ids, train=False):\n        self.ids = list(image_ids)\n        self.train = train\n\n        self.base_tf = T.Compose([\n            T.Resize(256),\n            T.CenterCrop(IMG_SIZE),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]),\n        ])\n\n        self.aug_tf = T.Compose([\n            T.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n            T.RandomHorizontalFlip(p=0.5),\n            T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n            T.RandomRotation(15),\n        ])\n\n    def crop_bbox(self, img, bbox):\n        x, y, w, h = map(int, bbox)\n        m = 0.15\n        x = max(0, int(x - w * m))\n        y = max(0, int(y - h * m))\n        w = int(w * (1 + 2 * m))\n        h = int(h * (1 + 2 * m))\n        return img.crop((x, y, x + w, y + h))\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n\n        path = os.path.join(IMAGES_DIR, img_paths[img_id])\n        img = Image.open(path).convert(\"RGB\")\n        img = self.crop_bbox(img, bboxes[img_id])\n\n        if self.train:\n            img = self.aug_tf(img)\n        img = self.base_tf(img)\n\n        label = labels[img_id]\n\n        return img, label\n\n# ---------------- DATASETS ----------------\nfull_train_dataset = CUBVisionDataset(train_ids, train=True)\nfull_val_dataset   = CUBVisionDataset(train_ids, train=False)\ntest_dataset       = CUBVisionDataset(test_ids,  train=False)\n\n# ---------------- STRATIFIED VAL SPLIT ----------------\nval_size_per_class = 3\n\nclass_to_indices = {}\nfor idx, img_id in enumerate(train_ids):\n    class_to_indices.setdefault(labels[img_id], []).append(idx)\n\ntrain_indices, val_indices = [], []\nfor cls, indices in class_to_indices.items():\n    random.shuffle(indices)\n    val_indices.extend(indices[:val_size_per_class])\n    train_indices.extend(indices[val_size_per_class:])\n\ntrain_dataset = Subset(full_train_dataset, train_indices)\nval_dataset   = Subset(full_val_dataset,   val_indices)\n\nprint(f\"Final splits ‚Üí Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n\n# ---------------- LOADERS ----------------\ntrain_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True,\n                          num_workers=4, pin_memory=True, persistent_workers=True)\nval_loader   = DataLoader(val_dataset,   BATCH_SIZE, shuffle=False,\n                          num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader  = DataLoader(test_dataset,  BATCH_SIZE, shuffle=False,\n                          num_workers=4, pin_memory=True, persistent_workers=True)\n\nprint(\"‚úì Vision-only dataset ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T13:35:26.492661Z","iopub.execute_input":"2026-02-15T13:35:26.493396Z","iopub.status.idle":"2026-02-15T13:35:31.620720Z","shell.execute_reply.started":"2026-02-15T13:35:26.493365Z","shell.execute_reply":"2026-02-15T13:35:31.619907Z"}},"outputs":[{"name":"stderr","text":"2026-02-15 13:35:26,734 - INFO - NumExpr defaulting to 4 threads.\n","output_type":"stream"},{"name":"stdout","text":"Official train samples: 5994\nOfficial test samples:  5794\nFinal splits ‚Üí Train: 5394, Val: 600, Test: 5794\n‚úì Vision-only dataset ready\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## ü§ç Cell 3: Model Definition (backbone)","metadata":{}},{"cell_type":"code","source":"# ====================== FULL VISION-ONLY PIPELINE ======================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport timm\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.cuda.amp import GradScaler, autocast\n\n# ---------------- CONFIG ----------------\nNUM_CLASSES = 200\nVISION_DIM = 1024        # Swin-B output dim\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ====================== MODEL ======================\nclass ViTMixSwin(nn.Module):\n    \"\"\"\n    Vision-only fine-grained classifier using Swin-B.\n    \"\"\"\n\n    def __init__(self, freeze_vision=True):\n        super().__init__()\n\n        # ---- Vision Encoder (Swin-B) ----\n        self.vision_encoder = timm.create_model(\n            \"swin_base_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0,       # remove classifier\n            global_pool=\"avg\"    # output (B, 1024)\n        )\n\n        if freeze_vision:\n            for p in self.vision_encoder.parameters():\n                p.requires_grad = False\n\n        # ---- Classification Head ----\n        self.classifier = nn.Sequential(\n            nn.Linear(VISION_DIM, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.3),\n            nn.Linear(512, NUM_CLASSES),\n        )\n\n    def forward(self, images):\n        v_feat = self.vision_encoder(images)\n        logits = self.classifier(v_feat)\n        return logits\n\n\n\n# ====================== USAGE EXAMPLE ======================\n# model = ViTMixSwin(freeze_vision=True).to(DEVICE)\n# optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=1e-4)\n# scheduler = CosineAnnealingLR(optimizer, T_max=20)\n# criterion = nn.CrossEntropyLoss()\n# history, best_state = train_model(model, train_loader, val_loader, optimizer, criterion, DEVICE,\n#                                   epochs=20, scheduler=scheduler, amp=True, unfreeze_after=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T13:35:31.621560Z","iopub.execute_input":"2026-02-15T13:35:31.621888Z","iopub.status.idle":"2026-02-15T13:35:32.225416Z","shell.execute_reply.started":"2026-02-15T13:35:31.621863Z","shell.execute_reply":"2026-02-15T13:35:32.224861Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## ## ü§ç Cell 4 Training Utilities","metadata":{}},{"cell_type":"code","source":"# ====================== METRICS ======================\n@torch.no_grad()\ndef accuracy(logits, targets, topk=(1,)):\n    maxk = max(topk)\n    batch_size = targets.size(0)\n    _, preds = logits.topk(maxk, dim=1, largest=True, sorted=True)\n    preds = preds.t()\n    correct = preds.eq(targets.view(1, -1).expand_as(preds))\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\nclass AverageMeter:\n    def __init__(self, name):\n        self.name = name\n        self.reset()\n    def reset(self):\n        self.val = self.sum = self.count = self.avg = 0.0\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n# ====================== TRAIN / VALIDATION ======================\ndef train_one_epoch(model, loader, optimizer, criterion, device, scaler=None):\n    model.train()\n    loss_meter = AverageMeter(\"train_loss\")\n    acc_meter = AverageMeter(\"train_acc1\")\n\n    for images, labels in tqdm(loader, desc=\"Train\", leave=False, colour=\"green\"):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        if scaler:\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        acc1 = accuracy(outputs, labels, topk=(1,))[0]\n        bs = images.size(0)\n        loss_meter.update(loss.item(), bs)\n        acc_meter.update(acc1.item(), bs)\n\n    return {\"train_loss\": loss_meter.avg, \"train_acc1\": acc_meter.avg}\n\n@torch.no_grad()\ndef validate_one_epoch(model, loader, criterion, device):\n    model.eval()\n    loss_meter = AverageMeter(\"val_loss\")\n    acc1_meter = AverageMeter(\"val_acc1\")\n    acc5_meter = AverageMeter(\"val_acc5\")\n\n    for images, labels in tqdm(loader, desc=\"Validate\", leave=False, colour=\"red\"):\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        acc1, acc5 = accuracy(outputs, labels, topk=(1,5))\n        bs = images.size(0)\n        loss_meter.update(loss.item(), bs)\n        acc1_meter.update(acc1.item(), bs)\n        acc5_meter.update(acc5.item(), bs)\n\n    return {\"val_loss\": loss_meter.avg, \"val_acc1\": acc1_meter.avg, \"val_acc5\": acc5_meter.avg}\n\n# ====================== FULL TRAINING LOOP ======================\ndef train_model(model, train_loader, val_loader, optimizer, criterion,\n                device, epochs=20, scheduler=None, amp=True, unfreeze_after=5):\n    \"\"\"\n    - amp: Use mixed precision\n    - unfreeze_after: epoch number to unfreeze last 2-3 Swin blocks for fine-tuning\n    \"\"\"\n    scaler = GradScaler() if amp else None\n    best_val_acc = 0.0\n    best_state = None\n    history = []\n\n    for epoch in range(1, epochs+1):\n        # Partial unfreeze\n        if epoch == unfreeze_after:\n            print(\">> Unfreezing last 2 Swin blocks for fine-tuning...\")\n            # Unfreeze last 2 layers\n            for name, param in model.vision_encoder.named_parameters():\n                if \"layers.2\" in name or \"layers.3\" in name:\n                    param.requires_grad = True\n\n        train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n        val_metrics = validate_one_epoch(model, val_loader, criterion, device)\n\n        if scheduler:\n            scheduler.step()\n\n        metrics = {**train_metrics, **val_metrics}\n        history.append(metrics)\n\n        if val_metrics[\"val_acc1\"] > best_val_acc:\n            best_val_acc = val_metrics[\"val_acc1\"]\n            best_state = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(),\n                          \"epoch\": epoch, \"val_acc1\": best_val_acc}\n\n        print(f\"Epoch [{epoch}/{epochs}] | \"\n              f\"Train Loss: {train_metrics['train_loss']:.4f}, Train Acc: {train_metrics['train_acc1']:.2f}% | \"\n              f\"Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc1']:.2f}%\")\n\n    return history, best_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T13:35:32.226121Z","iopub.execute_input":"2026-02-15T13:35:32.226362Z","iopub.status.idle":"2026-02-15T13:35:32.241345Z","shell.execute_reply.started":"2026-02-15T13:35:32.226336Z","shell.execute_reply":"2026-02-15T13:35:32.240839Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# STARTING TRAINING","metadata":{}},{"cell_type":"code","source":"model = ViTMixSwin(freeze_vision=True).to(DEVICE)\n\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=20)\ncriterion = nn.CrossEntropyLoss()\n\nhistory, best_state = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    device=DEVICE,\n    epochs=20,\n    scheduler=scheduler,\n    amp=True,\n    unfreeze_after=5\n)\n\n# -------------------- 4Ô∏è‚É£ Print best validation accuracy --------------------\nprint(f\"\\n‚úì Best validation accuracy: {best_state['val_acc1']:.2f}% at epoch {best_state['epoch']}\")\ntest_metrics = validate_one_epoch(\n    model=model, \n    loader=test_loader,\n    criterion=criterion,\n    device=DEVICE\n)\n\nprint(\"\\n=== TEST RESULTS ===\")\nprint(f\"Top-1 Accuracy: {test_metrics['val_acc1']:.2f}%\")\nprint(f\"Top-5 Accuracy: {test_metrics['val_acc5']:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T13:35:32.242266Z","iopub.execute_input":"2026-02-15T13:35:32.242494Z","iopub.status.idle":"2026-02-15T13:57:30.350110Z","shell.execute_reply.started":"2026-02-15T13:35:32.242471Z","shell.execute_reply":"2026-02-15T13:57:30.349120Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n2026-02-15 13:35:33,788 - INFO - Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fe356aedc75446ba3dd856864cb2b3f"}},"metadata":{}},{"name":"stderr","text":"2026-02-15 13:35:36,078 - INFO - [timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n/tmp/ipykernel_96/2713710453.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler() if amp else None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_96/2713710453.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [1/20] | Train Loss: 4.5727, Train Acc: 15.29% | Val Loss: 3.2585, Val Acc: 47.50%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [2/20] | Train Loss: 2.6757, Train Acc: 43.29% | Val Loss: 1.8326, Val Acc: 59.33%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [3/20] | Train Loss: 1.7754, Train Acc: 57.12% | Val Loss: 1.3305, Val Acc: 66.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [4/20] | Train Loss: 1.3967, Train Acc: 63.44% | Val Loss: 1.0900, Val Acc: 72.33%\n>> Unfreezing last 2 Swin blocks for fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [5/20] | Train Loss: 1.1722, Train Acc: 68.58% | Val Loss: 0.9543, Val Acc: 74.50%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [6/20] | Train Loss: 1.0218, Train Acc: 71.82% | Val Loss: 0.8629, Val Acc: 76.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [7/20] | Train Loss: 0.9128, Train Acc: 74.10% | Val Loss: 0.8010, Val Acc: 78.50%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [8/20] | Train Loss: 0.8333, Train Acc: 77.51% | Val Loss: 0.7477, Val Acc: 79.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [9/20] | Train Loss: 0.7752, Train Acc: 78.70% | Val Loss: 0.7120, Val Acc: 81.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [10/20] | Train Loss: 0.7326, Train Acc: 79.37% | Val Loss: 0.7011, Val Acc: 80.67%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [11/20] | Train Loss: 0.6942, Train Acc: 81.48% | Val Loss: 0.6741, Val Acc: 82.33%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [12/20] | Train Loss: 0.6595, Train Acc: 82.00% | Val Loss: 0.6683, Val Acc: 81.83%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [13/20] | Train Loss: 0.6310, Train Acc: 82.93% | Val Loss: 0.6454, Val Acc: 82.67%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [14/20] | Train Loss: 0.6199, Train Acc: 83.50% | Val Loss: 0.6335, Val Acc: 82.83%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [15/20] | Train Loss: 0.6060, Train Acc: 83.93% | Val Loss: 0.6264, Val Acc: 82.33%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d853882e673745119dabddb6f0fd139d"}},"metadata":{}},{"name":"stdout","text":"Epoch [16/20] | Train Loss: 0.5878, Train Acc: 83.83% | Val Loss: 0.6223, Val Acc: 83.17%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582da907424043d48d9a92b423108553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53e840482a143d7918867476e44092c"}},"metadata":{}},{"name":"stdout","text":"Epoch [17/20] | Train Loss: 0.5861, Train Acc: 84.15% | Val Loss: 0.6226, Val Acc: 83.50%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [18/20] | Train Loss: 0.5855, Train Acc: 84.39% | Val Loss: 0.6200, Val Acc: 83.67%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [19/20] | Train Loss: 0.5710, Train Acc: 84.56% | Val Loss: 0.6195, Val Acc: 83.67%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch [20/20] | Train Loss: 0.5688, Train Acc: 84.72% | Val Loss: 0.6195, Val Acc: 83.50%\n\n‚úì Best validation accuracy: 83.67% at epoch 19\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validate:   0%|          | 0/121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n=== TEST RESULTS ===\nTop-1 Accuracy: 84.48%\nTop-5 Accuracy: 98.38%\n","output_type":"stream"}],"execution_count":5}]}